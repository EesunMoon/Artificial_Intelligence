{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[2021AI]과제5_Cliff Walking with DQN",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoonEeSun/Artificial_Intelligence/blob/main/assignment/HW5_Cliff_Walking_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8Jl3uq0gg-E"
      },
      "source": [
        "# **[인공지능] 과제5 Cliff Walking 예제 DQN 구현**\n",
        "*   **DQN Class를 완성하여 결과를 살펴보는 것이 목표**입니다.\n",
        "*   기본적인 코드는 아래 노트에 모두 작성되어 있습니다. 비어있는 함수 부분을 완성하면 됩니다.\n",
        "*   **과제 수행 시 주의사항: 외부 라이브러리로 DQN 적용하지 말 것, 수업 때 배운 내용대로 DQN을 주어진 함수에 구현할 것.** 웹 상에 있는 다양한 DQN 코드를 참고하는 것은 괜찮습니다.\n",
        "*   **보고서 작성 내용**: 여러분이 완성한 DQN 알고리즘의 내용과 결과의 의미를 분석하는 내용을 작성하면 됩니다.\n",
        "작성한 코드와 실행 결과를 첨부하길 바라며, 코드에는 자세한 주석을 필수적으로 포함하기 바랍니다. 보고서는 PDF로 제출바랍니다.\n",
        "*   보고서는 12월 16일 오후 11시 59분까지 블랙보드에 보고서 형태로 제출하면 됩니다. 지각은 0점입니다.\n",
        "*   **Deep Sarsa를 추가로 구현하여 보고서에 관련 내용을 추가적으로 작성 및 제출할 경우 가산점 5점 부여**\n",
        "\n",
        "# **본 노트를 본인의 drive로 복사하여 활용하기 바랍니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8RhQiePceMv"
      },
      "source": [
        "본 과제 역시 이전과제와 동일한 환경을 사용합니다. 따라서 학습된 Q-value를 입력하면 해당하는 Q-value의 greedy 정책이 출력되도록 하는 QtoPolicy Class 또한 동일하게 사용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlwSKTZhgDTM"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict, namedtuple, deque\n",
        "from gym.envs.toy_text.cliffwalking import CliffWalkingEnv # Cliff Walking 환경\n",
        "\n",
        "# 신경망 구현을 위한 pytorch 라이브러리 import\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLfl84A7nfo7"
      },
      "source": [
        "# 학습된 Q-value를 입력하면 그 Q-value의 greedy 정책이 출력되도록 함\n",
        "# DQN이나 Deep Sarsa를 통해 학습된 정책을 출력하기 위해 필요\n",
        "class QtoPolicy:\n",
        "    def __init__(self):\n",
        "        self.action = ['↑', '→', '↓', '←', 'X'] # Agent의 가능한 Action\n",
        "\n",
        "    def printPolicy(self, Q):\n",
        "        # Agent의 정책 : 학습된 Q-value가 Q-table에 있으면 Q-value를 가장 크게 만드는 정책을 저장(greedy한 정책)\n",
        "        policy = np.array([np.argmax(Q[key]) if key in Q else -1 for key in np.arange(48)])\n",
        "        print('init policy=',policy)\n",
        "        # 가장 큰 Q-value : 학습된 Q-value가 Q-table에 있으면 Q-value 중 max값을, 없으면 0을 변수 v에 저장\n",
        "        v = ([np.max(Q[key]) if key in Q else 0 for key in np.arange(48)])\n",
        "        print('value =',v)\n",
        "        # Agent의 행동 저장\n",
        "        actions = np.stack([self.action for _ in range(len(policy))], axis=0)\n",
        "        print('actions =',actions)\n",
        "        # cliff walking 초기 환경 구축\n",
        "        policy[36:] = np.array([0] + [3] * 10 + [4])\n",
        "\n",
        "        print(np.take(actions, np.reshape(policy, (4, 12))))\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApP57w4wcq7l"
      },
      "source": [
        "DQN의 experience replay기법 구현을 위한 replay buffer class를 생성합니다. DQN에서는 (state, action, reward, next_state)가 저장됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aod3D2Lnkw7_"
      },
      "source": [
        "# Experience replay를 위한 replay buffer class 생성\n",
        "# agent가 관찰한 transition을 저장 - (state, action) pair을 (next_state, reward) 결과에 매핑\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'reward', 'next_state'))\n",
        "\n",
        "# replay buffer\n",
        "# bounded size의 cyclic buffer - 최근에 관찰한 transition을 가지고 있음\n",
        "class ReplayBuffer(object):\n",
        "\n",
        "    # 버퍼 초기화\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque([],maxlen=capacity)\n",
        "    \n",
        "    # 버퍼에 Agent의 경험 정보 추가\n",
        "    def push(self, *args):\n",
        "        self.buffer.append(Transition(*args))\n",
        "\n",
        "    # .sample() 메쏘드 : 학습을 위해 transition의 랜덤한 batch를 선택\n",
        "    # experience replay에서는 Agent의 경험 정보를 저장하는 replay buffer에서 \n",
        "    # 경험정보를 sampling하여 신경망 update에 사용\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    # 버퍼 길이 계산\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-1KCWUGc9Eu"
      },
      "source": [
        "DQN에서 Q-Network으로 사용할 신경망 모델을 PyTorch 기반으로 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moiYtfA0pYiY"
      },
      "source": [
        "# DQN은 Q-learning에서의 Q-table을 신경망으로 구성한 것\n",
        "# DQN의 Q-Network를 신경망으로 구현하기 때문에 DNN 선언\n",
        "class DNN(nn.Module):\n",
        "  # 신경망 초기화\n",
        "    def __init__(self, inputs, outputs):\n",
        "        super(DNN, self).__init__() \n",
        "        self.x_dim = inputs # input 차원\n",
        "        self.y_dim = outputs # output 차원\n",
        "        self.fc_variable_no = 100 # 층 개수\n",
        "\n",
        "        # network 용 변수\n",
        "        self.fc_in = nn.Linear(self.x_dim, self.fc_variable_no) # 입력층\n",
        "        self.fc_hidden1 = nn.Linear(self.fc_variable_no, self.fc_variable_no) # 은닉층1\n",
        "        self.fc_hidden2 = nn.Linear(self.fc_variable_no, self.fc_variable_no) # 은닉층2\n",
        "        self.fc_hidden3 = nn.Linear(self.fc_variable_no, self.fc_variable_no) # 은닉층3\n",
        "        self.fc_out = nn.Linear(self.fc_variable_no, self.y_dim) # 출력충\n",
        "        self.relu = nn.ReLU() # 활성함수 - ReLU(: DNN에서의 그레디언트 소멸 문제를 해결하기 위해 ReLU 활성함수 사용)\n",
        "    \n",
        "    # forward propagation\n",
        "    def forward(self, x):\n",
        "        x = torch.reshape(x, [-1, self.x_dim]) # 차원을 맞추기 위해 reshape\n",
        "        x = self.relu(self.fc_in(x)) # 입력층 \n",
        "        x = self.relu(self.fc_hidden1(x)) # 은닉층1\n",
        "        x = self.relu(self.fc_hidden2(x)) # 은닉층2\n",
        "        x = self.relu(self.fc_hidden3(x)) # 은닉층3\n",
        "        x = self.fc_out(x) # 출력층\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFM4zUEndWxt"
      },
      "source": [
        "DQN 알고리즘 class를 정의합니다. 하이퍼파라미터는 주어진 값을 사용하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR9Mn8MoxOvS"
      },
      "source": [
        "# DQN 알고리즘 class\n",
        "class DQN:\n",
        "    def __init__(self):\n",
        "        self.state_no = 48 # 상태 수\n",
        "        self.action_no = 4 # action 수\n",
        "        self.alpha = 0.001 # learning rate\n",
        "        self.gamma = 0.99 # discount vector\n",
        "        self.epsilon = 0.2 # 앱실론\n",
        "\n",
        "        self.batch_size = 32  # Experience replay에서의 batch size\n",
        "        self.training_interval = 10  # main Q-Network 학습 interval\n",
        "        self.target_update_interval = 100  # target Q-Network 학습 interval\n",
        "        # 100번에 한번씩 target계산을 위한 target Q-Network를 main Q-Network로 update\n",
        "\n",
        "        # fixed target Q-value를 위한 main_network, target_network\n",
        "        self.main_net = DNN(self.state_no, self.action_no) \n",
        "        self.target_net = DNN(self.state_no, self.action_no) # target 계신용\n",
        "\n",
        "        # target Q-network를 main Q-network와 동일하게 초기화\n",
        "        self.target_net.load_state_dict(self.main_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        # 최적화 함수 정의\n",
        "        self.optimizer = optim.Adam(self.main_net.parameters(), lr=self.alpha)\n",
        "        # Experience replay를 위한 buffer 정의\n",
        "        self.buffer = ReplayBuffer(500)\n",
        "\n",
        "    # state의 인덱스가 연속적인 의미를 가지고 있지 않으므로 효과적인 학습을\n",
        "    # 위해 one-hot encoding을 수행\n",
        "    def one_hot_state(self, state):\n",
        "        one_hot_encoded = np.zeros((1, self.state_no))\n",
        "        one_hot_encoded[0, state] = 1\n",
        "\n",
        "        return one_hot_encoded\n",
        "    \n",
        "    # 학습이 끝난 후 Q-Network에서 Q-value 계산하는 함수\n",
        "    def get_q_values(self):\n",
        "        q_values = defaultdict(lambda: [0.0] * self.action_no)\n",
        "        # 각 state 별 Q-value 계산\n",
        "        for i in range(self.state_no):\n",
        "            state = torch.tensor(self.one_hot_state(i)).float()\n",
        "            q_values[i] = self.main_net(state).tolist()\n",
        "        return q_values\n",
        "\n",
        "    # 신경망 최적화 모델\n",
        "    def optimize_model(self):\n",
        "        # Experience replay를 위한 buffer의 크기가 Experience replay에서의 batch size보다 작으면 return\n",
        "        if len(self.buffer) < self.batch_size:\n",
        "            return\n",
        "        \n",
        "        # Experience replay : 학습을 위해 transition의 랜덤한 batch를 선택\n",
        "        transitions = self.buffer.sample(self.batch_size)\n",
        "\n",
        "        # Transpose the batch \n",
        "        # - Transition의 배치배열을 배치배열의 Transition으로 전환 \n",
        "        # :: Transition - ('state', 'action', 'reward', 'next_state')\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        # 최종상태가 아닌 state의 mask를 계산 & 배치 elements 연결 \n",
        "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), dtype=torch.bool)\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None]) \n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "        # Q(s, a) 계산 - target 계산이 아니므로 main_net을 이용\n",
        "        # - 모델이 Q(s)를 계산하고, 취해지는 action의 열을 선택\n",
        "        # (main_Q-network에 따라 각각의 batch state에 대해 취해지는 action)\n",
        "        state_action_values = self.main_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "        # reward + gamma*maxa(S', a' : w) 계산(target value)\n",
        "        # Fixed target Q-value : target value이기 때문에 target net을 이용하여 계산 수행\n",
        "        # 모든 다음 state에 대해 Q(S') 계산 (next_stat_values : maxa(Q(s')))\n",
        "        # - 'non_final_values'의 다음 기대 action의 값은 older target net에 의해서 계산됨 (가장 높은 reward를 선택)\n",
        "        # mask를 기반으로 병합되므로, state가 최종일 경우 예상 state 값 or 0을 가질 수 있다.\n",
        "        next_state_values = torch.zeros(self.batch_size)\n",
        "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
        "        # target Q values 계산\n",
        "        target_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
        "\n",
        "        ## Compute Huber loss\n",
        "        # 손실함수 정의\n",
        "        criterion = nn.SmoothL1Loss()\n",
        "        # 오류 계산 :: (target value - Q(s,a))\n",
        "        loss = criterion(state_action_values, target_state_action_values)\n",
        "        ## Optimize the model\n",
        "        # 최적화를 위해 gradient 0으로 초기화\n",
        "        self.optimizer.zero_grad()\n",
        "        # 오류 역전파\n",
        "        loss.backward()\n",
        "        for param in self.main_net.parameters():\n",
        "            param.grad.data.clamp_(-10, 10)\n",
        "        # gradient 갱신\n",
        "        self.optimizer.step()\n",
        "\n",
        "    # DQN 갱신 부분\n",
        "    def update(self, state, action, reward, next_state, time_step):\n",
        "      # memory buffer안에 transition 정보 저장\n",
        "        self.buffer.push(torch.from_numpy(state).float(),\n",
        "                         torch.tensor(action).reshape((-1, 1)),\n",
        "                         torch.tensor(reward).reshape((-1, 1)),\n",
        "                         torch.from_numpy(next_state).float())\n",
        "\n",
        "        # training interval 마다 신경망 최적화 수행\n",
        "        if (time_step + 1) % self.training_interval == 0:\n",
        "            self.optimize_model()\n",
        "        \n",
        "        # target update interval 마다 target Q-network를 main Q-network 값으로 update\n",
        "        if (time_step + 1) % self.target_update_interval == 0:\n",
        "            self.target_net.load_state_dict(self.main_net.state_dict())\n",
        "\n",
        "    # DQN epsilon-greedy 정책 - behavior policy(실제 행동 수행)\n",
        "    def act(self, state):\n",
        "      # np.random.rand() : 0과 1사이의 균일분포에서 action_no(4) 크기의 난수 matrix\n",
        "        # 숫자가 epsilon보다 작을 경우 랜덤하게 action을 선택\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            action = np.random.choice(self.action_no)\n",
        "        # 숫자가 epsilon보다 크거나 같을 경우 q-value을 최대화하는 action을 선택\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "              state  = torch.from_numpy(state).float() # 형식을 맞춰주기 위해 tensor형식으로 바꿈\n",
        "              q_values = self.main_net(state)\n",
        "              action = torch.argmax(q_values, 1).item() # 선택할 수 있는 1x48가지 경우에서 1차원으로 action을 빼내기\n",
        "              # torch.argmax(q_values, 1)의 output은 tensor([~]) 형태이므로 안의 내용을 빼오려면 .item()을 써야함.\n",
        "        return action\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0UQRaPBknAI"
      },
      "source": [
        "이전 과제와 동일하게 OpenAI Gym에서의 Cliff Walking 환경을 로드하고, 주어진 Q-value에서 greedy policy를 출력하는 QtoPolicy Class를 정의합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orqk6rcLg-uZ"
      },
      "source": [
        "env = CliffWalkingEnv() # Cliff Walking 환경 load\n",
        "policy = QtoPolicy() # Q-value의 greedy policy를 출력하는 OtoPolicy 클래스 정의"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMTSRYKNk1Ke"
      },
      "source": [
        "DQN Class를 정의하고 5000 episode 동안 학습을 수행합니다.\n",
        "\n",
        "DQN에서의 training interval 및 target Q-network update interval을 위해 episode와 관계없는 time-step을 사용하여 DQN class에서 활용할 수 있게 해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ5ZtkU9xJGg",
        "outputId": "443ed36b-d446-45ae-d306-7b568ff62a53"
      },
      "source": [
        "agent_DQN = DQN() # DQN 클래스 정의\n",
        "time_step = 0\n",
        "# 5000 episode 동안 학습 수행\n",
        "for ep in tqdm(range(5000)):\n",
        "    done = False\n",
        "    # reset() : 상태 초기화\n",
        "    state = env.reset()\n",
        "\n",
        "    state = agent_DQN.one_hot_state(state) # state 원 핫 인코드 수행\n",
        "    action = agent_DQN.act(state) # DQN action 고르기 (epsilon-greedy 정책 - behavior policy)\n",
        "    # print(action)\n",
        "\n",
        "    ep_reward = 0 # 에피소드 한 번의 reward\n",
        "    ep_steps = 0\n",
        "    while not done:\n",
        "        # 알고리즘을 통해 얻어낸 Action을 1 step 수행한 후, 현재 state와 reward 등의 정보를 반환\n",
        "        # Step()함수의 return : tuple 형태의 (next_state, reward, done, info)를 반환\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        reward = reward \n",
        "        # state one-hot encoding 수행\n",
        "        next_state = agent_DQN.one_hot_state(next_state)\n",
        "\n",
        "        # 앱실론 greedy 정책으로 다음 action choose\n",
        "        next_action = agent_DQN.act(next_state)\n",
        "\n",
        "        # 가치함수 update - target net 이용\n",
        "        agent_DQN.update(state, action, reward, next_state, time_step)\n",
        "        time_step = time_step + 1\n",
        "\n",
        "        ep_reward+=reward # episode 별 reward 계산\n",
        "        state = next_state # 상태 update\n",
        "        action = next_action # action update\n",
        "        ep_steps = ep_steps + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:912: UserWarning: Using a target size (torch.Size([32, 32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n",
            "100%|██████████| 5000/5000 [13:33<00:00,  6.14it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHXWWXQTlL17"
      },
      "source": [
        "학습된 Q-value를 이용하여 학습된 정책을 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqHh3TRcs3dh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0977b00-2fde-49b2-dedc-b27f7a8af307"
      },
      "source": [
        "print('Learned policy by DQN')\n",
        "policy.printPolicy(agent_DQN.get_q_values())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned policy by DQN\n",
            "init policy= [2 2 2 2 2 2 2 1 2 2 2 2 2 1 2 2 2 1 1 1 1 1 2 2 2 1 1 1 1 1 1 1 1 1 1 2 2\n",
            " 1 1 1 1 1 1 1 1 1 1 1]\n",
            "value = [-89.07472229003906, -89.05058288574219, -89.07815551757812, -88.93586730957031, -89.28718566894531, -89.19509887695312, -89.16865539550781, -88.76815032958984, -88.97096252441406, -89.09000396728516, -89.26597595214844, -89.02621459960938, -88.98213195800781, -89.68174743652344, -88.83232116699219, -89.06253814697266, -88.73747253417969, -89.40100860595703, -88.84500885009766, -89.107421875, -89.46558380126953, -89.17556762695312, -89.125, -89.03252410888672, -89.08116149902344, -88.90898895263672, -89.01132202148438, -89.02355194091797, -89.10859680175781, -89.16790008544922, -89.07036590576172, -89.3904800415039, -89.06970977783203, -89.14940643310547, -89.08528900146484, -89.05450439453125, -89.06951904296875, -74.77819061279297, -71.58032989501953, -69.44672393798828, -67.49089813232422, -71.4187240600586, -74.56720733642578, -73.8974838256836, -72.92427062988281, -70.03270721435547, -71.80157470703125, -70.46398162841797]\n",
            "actions = [['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']]\n",
            "[['↓' '↓' '↓' '↓' '↓' '↓' '↓' '→' '↓' '↓' '↓' '↓']\n",
            " ['↓' '→' '↓' '↓' '↓' '→' '→' '→' '→' '→' '↓' '↓']\n",
            " ['↓' '→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '↓']\n",
            " ['↑' '←' '←' '←' '←' '←' '←' '←' '←' '←' '←' 'X']]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9IGh-KqT4Fz",
        "outputId": "ad516a9d-e6e4-4871-dd80-308f0948b812"
      },
      "source": [
        "print('DQN Total reward :')\n",
        "print(ep_reward)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DQN Total reward :\n",
            "-398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# deep sarsa에서 Experience replay를 위한 replay buffer class 생성\n",
        "# DQNd의 replay buffer에서 next action 정보 추가\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'reward', 'next_state', 'next_action'))\n",
        "\n",
        "# replay buffer\n",
        "# bounded size의 cyclic buffer - 최근에 관찰한 transition을 가지고 있음\n",
        "class ReplayBuffer(object):\n",
        "\n",
        "    # 버퍼 초기화\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque([],maxlen=capacity)\n",
        "    \n",
        "    # 버퍼에 Agent의 경험 정보 추가\n",
        "    def push(self, *args):\n",
        "        self.buffer.append(Transition(*args))\n",
        "\n",
        "    # .sample() 메쏘드 : 학습을 위해 transition의 랜덤한 batch를 선택\n",
        "    # experience replay에서는 Agent의 경험 정보를 저장하는 replay buffer에서 \n",
        "    # 경험정보를 sampling하여 신경망 update에 사용\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    # 버퍼 길이 계산\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "MM5q5kfU_SZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1mwvUqDcYDL"
      },
      "source": [
        "Deep Sarsa 알고리즘 class 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfLv4u54cXtM"
      },
      "source": [
        "# Deep Sarsa 알고리즘 class\n",
        "class DeepSarsa:\n",
        "    def __init__(self):\n",
        "        self.state_no = 48 # 상태 수\n",
        "        self.action_no = 4 # action 수\n",
        "        self.alpha = 0.001 # learning rate\n",
        "        self.gamma = 0.99 # discount vector\n",
        "        self.epsilon = 0.2 # 앱실론\n",
        "\n",
        "        self.batch_size = 32  # Experience replay에서의 batch size\n",
        "        self.training_interval = 10  # Q-Network 학습 interval\n",
        "        self.target_update_interval = 100  # target Q-Network 학습 interval\n",
        "\n",
        "         # fixed target Q-value를 위한 main_network, target_network\n",
        "        self.main_net = DNN(self.state_no, self.action_no)\n",
        "        self.target_net = DNN(self.state_no, self.action_no)\n",
        "\n",
        "        # target Q-network를 main Q-network와 동일하게 초기화\n",
        "        self.target_net.load_state_dict(self.main_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        # 최적화 함수\n",
        "        self.optimizer = optim.Adam(self.main_net.parameters(), lr=self.alpha)\n",
        "        # Experience replay를 위한 replay buffer 정의\n",
        "        self.buffer = ReplayBuffer(500)\n",
        "\n",
        "    # state의 인덱스가 연속적인 의미를 가지고 있지 않으므로 효과적인 학습을\n",
        "    # 위해 one-hot encoding을 수행\n",
        "    def one_hot_state(self, state):\n",
        "        one_hot_encoded = np.zeros((1, self.state_no))\n",
        "        one_hot_encoded[0, state] = 1\n",
        "\n",
        "        return one_hot_encoded\n",
        "    \n",
        "    # 학습이 끝난 후 Q-Network에서 Q-value 계산하는 함수\n",
        "    def get_q_values(self):\n",
        "        q_values = defaultdict(lambda: [0.0] * self.action_no)\n",
        "        # 각 state 별 Q-value 계산\n",
        "        for i in range(self.state_no):\n",
        "            state = torch.tensor(self.one_hot_state(i)).float()\n",
        "            q_values[i] = self.main_net(state).tolist()\n",
        "        return q_values\n",
        "\n",
        "    # 신경망 최적화 모델\n",
        "    def optimize_model(self):\n",
        "        # Experience replay를 위한 buffer의 크기가 Experience replay에서의 batch size보다 작으면 return\n",
        "        if len(self.buffer) < self.batch_size:\n",
        "            return\n",
        "        \n",
        "        # 학습을 위해 transition의 랜덤한 batch를 선택\n",
        "        transitions = self.buffer.sample(self.batch_size)\n",
        "\n",
        "        # Transpose the batch \n",
        "        # - Transition의 배치배열을 배치배열의 Transition으로 전환 \n",
        "        # :: Transition - ('state', 'action', 'reward', 'next_state', 'next_action')\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        # 최종상태가 아닌 state의 mask를 계산 & 배치 elements 연결 \n",
        "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), dtype=torch.bool)\n",
        "        # 연결\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None]) \n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "        next_action_batch = torch.cat(batch.next_action)\n",
        "\n",
        "        # Q(s, a) 계산 : target value가 아니므로 main net 사용\n",
        "        # - 모델이 Q(s)를 계산하고, 취해지는 action의 열을 선택\n",
        "        # (main_Q-network에 따라 각각의 batch state에 대해 취해지는 action)\n",
        "        state_action_values = self.main_net(state_batch).gather(1, action_batch)  \n",
        "\n",
        "        # target value 계산 : target net 이용\n",
        "        # next_state_values : Q(S', A')\n",
        "        next_state_values = self.target_net(non_final_next_states).gather(1, next_action_batch) ##\n",
        "        # Compute the expected Q values\n",
        "        target_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
        "\n",
        "        ## Compute Huber loss\n",
        "        # 손실함수 정의\n",
        "        criterion = nn.SmoothL1Loss()\n",
        "        # 오류 계산 :: (target value - Q(s,a))\n",
        "        loss = criterion(state_action_values, target_state_action_values)\n",
        "        ## Optimize the model\n",
        "        # 최적화를 위해 gradient 0으로 초기화\n",
        "        self.optimizer.zero_grad()\n",
        "        # 오류 역전파\n",
        "        loss.backward()\n",
        "        for param in self.main_net.parameters():\n",
        "            param.grad.data.clamp_(-10, 10)\n",
        "        # gradient 갱신\n",
        "        self.optimizer.step()\n",
        "\n",
        "    # DQN 갱신 부분\n",
        "    def update(self, state, action, reward, next_state, next_action, time_step):\n",
        "      # memory buffer안에 transition 저장\n",
        "        self.buffer.push(torch.from_numpy(state).float(),\n",
        "                         torch.tensor(action).reshape((-1, 1)),\n",
        "                         torch.tensor(reward).reshape((-1, 1)),\n",
        "                         torch.from_numpy(next_state).float(),\n",
        "                         torch.tensor(next_action).reshape((-1, 1)))\n",
        "                         \n",
        "\n",
        "        # training interval 마다 신경망 최적화 수행\n",
        "        # Deep Sarsa에서는 앱실론 greedy로 예측한 다음 action의 정보가 있어야 갱신 가능\n",
        "        if (time_step + 1) % self.training_interval == 0:\n",
        "            self.optimize_model()\n",
        "        \n",
        "        # target update interval 마다 target network를 main network의 값으로 update\n",
        "        if (time_step + 1) % self.target_update_interval == 0:\n",
        "            self.target_net.load_state_dict(self.main_net.state_dict())\n",
        "\n",
        "    # DQN epsilon-greedy 정책\n",
        "    def act(self, state):\n",
        "      # np.random.rand() : 0과 1사이의 균일분포에서 action_no(4) 크기의 난수 matrix\n",
        "        # 숫자가 epsilon보다 작을 경우 랜덤하게 action을 선택\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            action = np.random.choice(self.action_no)\n",
        "        # 숫자가 epsilon보다 크거나 같을 경우 q-value을 최대화하는 action을 선택\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "              # 내가 작성\n",
        "              state  = torch.from_numpy(state).float() # 형식을 맞춰주기 위해 tensor형식으로 바꿈\n",
        "              q_values = self.main_net(state)\n",
        "              action = torch.argmax(q_values, 1).item() # 선택할 수 있는 1x48가지 경우에서 1차원으로 action을 빼내기\n",
        "              # torch.argmax(q_values, 1)의 output은 tensor([~]) 형태이므로 안의 내용을 빼오려면 .item()을 써야함.\n",
        "        return action\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa-hymqGclIl"
      },
      "source": [
        "Deep Sarsa Class를 정의하고 5000 episode 동안 학습을 수행합니다.\n",
        "\n",
        "Deep Sarsa에서의 training interval 및 target Q-network update interval을 위해 episode와 관계없는 time-step을 사용하여 DQN class에서 활용할 수 있게 해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLA8Ye_qckVi",
        "outputId": "4705b6d6-6a31-4381-f8cc-004667c270c6"
      },
      "source": [
        "agent_DSa = DeepSarsa() # Deep Sarsa 클래스 정의\n",
        "time_step = 0\n",
        "# 5000 episode 동안 학습 수행\n",
        "for ep in tqdm(range(5000)):\n",
        "    done = False\n",
        "    # reset() : 상태 초기화 - Step을 실행하다가 epsiode가 끝나서 이를 초기화해서 재시작해야할 때, 초기 State를 반환\n",
        "    state = env.reset()\n",
        "\n",
        "    state = agent_DSa.one_hot_state(state) # state 원핫인코드 수행\n",
        "    action = agent_DSa.act(state) # epsilon greedy policy로 action 고르기\n",
        "    # print(action)\n",
        "\n",
        "    ep_reward = 0\n",
        "    ep_steps = 0\n",
        "    while not done:\n",
        "        # 알고리즘을 통해 얻어낸 Action을 1 step 수행한 후, 현재 state와 reward 등의 정보를 반환\n",
        "        # Step()함수의 return : tuple 형태의 (next_state, reward, done, info)를 반환\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        reward = reward\n",
        "        # state one-hot encoding 수행\n",
        "        next_state = agent_DSa.one_hot_state(next_state)\n",
        "\n",
        "        # 앱실론 greedy 정책으로 다음 action choose\n",
        "        next_action = agent_DSa.act(next_state)\n",
        "\n",
        "        # 가치함수 update - Sarsa에서는 다음 action 정보가 필요\n",
        "        agent_DSa.update(state, action, reward, next_state, next_action, time_step)\n",
        "        time_step = time_step + 1\n",
        "\n",
        "        ep_reward+=reward\n",
        "        state = next_state # 상태 update\n",
        "        action = next_action # action update\n",
        "        ep_steps = ep_steps + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [03:08<00:00, 26.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgrrj0IDc7jC"
      },
      "source": [
        "학습된 Q-value를 이용하여 학습된 정책을 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n71Vvu3ZcXbf",
        "outputId": "d594969b-25a0-453c-e199-b8df570d1bcb"
      },
      "source": [
        "print('Learned policy by Deep Sarsa')\n",
        "policy.printPolicy(agent_DSa.get_q_values())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned policy by Deep Sarsa\n",
            "init policy= [1 2 1 1 1 1 1 1 2 1 1 2 2 1 1 2 1 1 2 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 0\n",
            " 1 1 1 1 1 1 1 1 1 1 1]\n",
            "value = [-55.829227447509766, -55.10524368286133, -56.29279708862305, -55.92826461791992, -54.66456604003906, -52.688968658447266, -52.884456634521484, -51.41745376586914, -49.75448226928711, -50.210960388183594, -49.3790397644043, -49.28713607788086, -56.17568588256836, -56.72223663330078, -55.17209243774414, -53.9659538269043, -54.203346252441406, -53.01736068725586, -51.52580642700195, -50.77650451660156, -49.760494232177734, -49.87345504760742, -49.49148178100586, -47.81624221801758, -55.50003433227539, -54.16746139526367, -53.3201789855957, -52.9602165222168, -52.453773498535156, -51.424720764160156, -50.747413635253906, -49.953731536865234, -49.25376510620117, -48.68437194824219, -48.53280258178711, -47.27762985229492, -56.830406188964844, -48.062984466552734, -45.29643249511719, -44.869293212890625, -43.04885482788086, -46.00102615356445, -44.56443405151367, -44.086944580078125, -47.20085525512695, -46.589481353759766, -43.792484283447266, -47.08938980102539]\n",
            "actions = [['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']\n",
            " ['↑' '→' '↓' '←' 'X']]\n",
            "[['→' '↓' '→' '→' '→' '→' '→' '→' '↓' '→' '→' '↓']\n",
            " ['↓' '→' '→' '↓' '→' '→' '↓' '↓' '→' '→' '→' '↓']\n",
            " ['→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '↓']\n",
            " ['↑' '←' '←' '←' '←' '←' '←' '←' '←' '←' '←' 'X']]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv4WIv5Mc_vM",
        "outputId": "e33e440f-68f0-4fa1-c932-e2cd44e1952a"
      },
      "source": [
        "print('Deep Sarsa Total reward :')\n",
        "print(ep_reward)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Sarsa Total reward :\n",
            "-36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NFlcKhvoq4u"
      },
      "source": [
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7bTDRcH9kra"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}