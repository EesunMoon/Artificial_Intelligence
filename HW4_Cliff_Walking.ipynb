{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[2021AI]과제4_Cliff Walking Example_19011773",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoonEeSun/Artificial_Intelligence/blob/main/HW4_Cliff_Walking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8Jl3uq0gg-E"
      },
      "source": [
        "# **[인공지능] 과제4 Cliff Walking 예제 구현**\n",
        "*   **QLearning Class 및 Sarsa Class를 완성하여 결과를 살펴보는 것이 목표**입니다.\n",
        "*   기본적인 코드는 아래 노트에 모두 작성되어 있습니다. 비어있는 함수 부분을 완성하면 됩니다.\n",
        "*   **과제 수행 시 주의사항: 외부 라이브러리로 Q-learning 및 Sarsa 적용하지 말 것, 수업 때 배운 내용대로 Q-learning과 Sarsa를 주어진 함수에 구현할 것.** 웹 상에 있는 다양한 Q-learning 및 Sarsa 코드를 참고하는 것은 괜찮습니다.\n",
        "*   **보고서 작성 내용**: 여러분이 완성한 Q-learning 및 sarsa 알고리즘의 내용과 결과의 의미를 분석하는 내용을 작성하면 됩니다.\n",
        "작성한 코드와 실행 결과를 첨부하길 바라며, 코드에는 자세한 주석을 필수적으로 포함하기 바랍니다. 보고서는 PDF로 제출바랍니다.\n",
        "*   보고서는 11월 30일 오후 11시 59분까지 블랙보드에 보고서 형태로 제출하면 됩니다. 지각은 0점입니다.\n",
        "\n",
        "# **본 노트를 본인의 drive로 복사하여 활용하기 바랍니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuK87dbPv7hW"
      },
      "source": [
        "본 과제는 OpenAI Gym 환경에 기반하여 작성되었습니다. Gym 라이브러리는 학습을 적용할 수 있는 다양한 환경을 제공합니다. 여기서는 수업에서 다뤘던 Cliff Walking 환경을 활용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlwSKTZhgDTM"
      },
      "source": [
        "import gym\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import random\n",
        "import itertools # looping을 위해\n",
        "import sys\n",
        "from collections import defaultdict # dictionary의 key 값이 없을 경우 미리 지정해놓은 초기(default)값을 반환\n",
        "from gym.envs.toy_text.cliffwalking import CliffWalkingEnv # Cliff Walking 환경"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4XRfMCWq9GL"
      },
      "source": [
        "QtoPolicy Class는 학습된 Q-value를 입력하면 해당하는 Q-value의 greedy 정책이 출력되도록 하는 함수 `printPolicy`를 구성하는 Class입니다. Q-learning 및 Sarsa를 이용하여 학습된 정책을 출력하기 위해 필요합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLfl84A7nfo7"
      },
      "source": [
        "# 학습된 Q-value를 입력하면 그 Q-value의 greedy 정책이 출력되도록 함\n",
        "# Q-learning이나 Sarsa를 통해 학습된 정책을 출력하기 위해 필요\n",
        "class QtoPolicy:\n",
        "    def __init__(self):\n",
        "        self.action = ['↑','→','↓','←'] # Agent의 가능한 Action\n",
        "    \n",
        "    def printPolicy(self, Q):\n",
        "        # Agent의 정책 : 학습된 Q-value가 Q-table에 있으면 Q-value를 가장 크게 만드는 정책을 저장(greedy한 정책)\n",
        "        policy = np.array([np.argmax(Q[key]) if key in Q else -1 for key in np.arange(48)]) \n",
        "        # 가장 큰 Q-value : 학습된 Q-value가 Q-table에 있으면 Q-value 중 max값을, 없으면 0을 변수 v에 저장\n",
        "        v = ([np.max(Q[key]) if key in Q else 0 for key in np.arange(48)])\n",
        "        # Agent의 행동 저장\n",
        "        actions = np.stack([self.action for _ in range(len(policy))], axis=0)\n",
        "        \n",
        "        # Agent의 행동에 따른 정책 출력\n",
        "        print(np.take(actions, np.reshape(policy, (4, 12))))\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV-qggQM8Apl"
      },
      "source": [
        "# class QLearning:\n",
        "#   # Q(s, a) 임의로 초기화\n",
        "#     def __init__(self):\n",
        "#         self.action_no = 4 # 행동 가짓수\n",
        "#         self.alpha = 0.01 # learning rate\n",
        "#         self.gamma = 0.9 # discount vector\n",
        "#         self.epsilon = 0.2 # 앱실론\n",
        "#         self.q_values = defaultdict(lambda: [0.0] * self.action_no) # value function : 누적보상\n",
        "        \n",
        "#     # update : Q-value Update (state(S), action(A), reward(R), next_state(S'), next_action(A') 주어짐)\n",
        "#     def update(self, state, action, reward, next_state, next_action):\n",
        "#         ## 함수를 완성하시오.\n",
        "#         # Choose action        \n",
        "#         # action = egreedy_policy(q_values, state, epsilon=0.1)\n",
        "#         # Do the action\n",
        "#         # next_state, reward, done = env.step(action)\n",
        "#         # Update q_values     \n",
        "#         # td_target = reward + self.gamma * np.max(q_values[next_state])   \n",
        "#         td_target = reward + self.gamma * np.max(self.q_values[next_state][action])\n",
        "#         td_error = td_target - self.q_values[state][action]\n",
        "#         self.q_values[state][action] += self.alpha * td_error\n",
        "#         # Update state\n",
        "#         # state = next_state\n",
        "    \n",
        "#     # act : 앱실론 greedy에 따라 action 선택\n",
        "#     def act(self, state):\n",
        "#         ## 함수를 완성하시오.\n",
        "#         # Get a random number from a uniform distribution between 0 and 1,\n",
        "#         # if the number is lower than epsilon choose a random action\n",
        "#         if np.random.random() < self.epsilon:\n",
        "#           action = np.random.choice(4)\n",
        "#         # Else choose the action with the highest value\n",
        "#         else:\n",
        "#           action = np.argmax(self.q_values[state]) # Choose the action with largest Q-value (state value)\n",
        "#         return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "761Rn8P2rPR7"
      },
      "source": [
        "아래는 Q-learning 알고리즘을 수행하는 Class의 정의입니다.\n",
        "*   `update()` 메쏘드의 경우 state, action, reward, next_state, next_action이 주어졌을 때 Q-value를 업데이트하는 함수입니다.\n",
        "*   `act()` 메쏘드의 경우 $\\epsilon$-greedy 정책에 따라 action을 선택하는 함수입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aod3D2Lnkw7_"
      },
      "source": [
        "class QLearning:\n",
        "  # Q(s, a) 임의로 초기화\n",
        "    def __init__(self):\n",
        "        self.action_no = 4 # 행동 가짓수\n",
        "        self.alpha = 0.01 # learning rate\n",
        "        self.gamma = 0.9 # discount vector\n",
        "        self.epsilon = 0.2 # 앱실론\n",
        "        self.q_values = defaultdict(lambda: [0.0] * self.action_no) # value function : 누적보상\n",
        "        \n",
        "    # update : Q-value Update (state(S), action(A), reward(R), next_state(S'), next_action(A') 주어짐)\n",
        "    def update(self, state, action, reward, next_state, next_action): \n",
        "      # Q(S, A) <- Q(S, A) + alpha*[R + gamma*maxa(Q(S', a))-Q(S, A)]\n",
        "        q_target = reward + self.gamma * np.max(self.q_values[next_state])\n",
        "        q_error = q_target - self.q_values[state][action]\n",
        "        self.q_values[state][action] += self.alpha * q_error\n",
        "    \n",
        "    # act : 앱실론 greedy에 따라 action 선택\n",
        "    def act(self, state):\n",
        "        # np.random.random() : 0과 1사이의 uniform distribution에서부터 random한 숫자를 가져오기\n",
        "        # 숫자가 epsilon보다 작을 경우 랜덤하게 action을 선택\n",
        "        if np.random.random() < self.epsilon:\n",
        "          action = np.random.choice(4)\n",
        "        # 숫자가 epsilon보다 크거나 같을 경우 q-value을 최대화하는 action을 선택\n",
        "        else:\n",
        "          action = np.argmax(self.q_values[state])\n",
        "        return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0Jq-C_Vr1HV"
      },
      "source": [
        "아래는 Sarsa 알고리즘을 수행하는 Class의 정의입니다.\n",
        "*   `update()` 메쏘드의 경우 state, action, reward, next_state, next_action이 주어졌을 때 Q-value를 업데이트하는 함수입니다.\n",
        "*   `act()` 메쏘드의 경우 $\\epsilon$-greedy 정책에 따라 action을 선택하는 함수입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moiYtfA0pYiY"
      },
      "source": [
        "class Sarsa:\n",
        "    def __init__(self):\n",
        "        self.action_no = 4 # 행동의 가짓수\n",
        "        self.alpha = 0.01 # learning rate\n",
        "        self.gamma = 0.9 # discount vector\n",
        "        self.epsilon = 0.2 # 앱실론\n",
        "        self.q_values = defaultdict(lambda: [0.0] * self.action_no) # 가치함수\n",
        "        \n",
        "    # update : Q-value update (state, action, reward, next_tate, next_action 주어짐)\n",
        "    def update(self, state, action, reward, next_state, next_action):\n",
        "        # Q(S,A) <- Q(S,A) + alpha*[R+gamma*Q(S',A')-Q(S,A)]\n",
        "        q_target = reward + self.gamma * self.q_values[next_state][next_action]\n",
        "        q_error = q_target - self.q_values[state][action]\n",
        "        self.q_values[state][action] += self.alpha * q_error\n",
        "        \n",
        "    # act : 앱실론 greedy에 따라 action 선택\n",
        "    def act(self, state):\n",
        "        # np.random.random() : 0과 1사이의 uniform distribution에서부터 random한 숫자를 가져오기\n",
        "        # 숫자가 epsilon보다 작을 경우 랜덤하게 action을 선택\n",
        "        if np.random.random() < self.epsilon:\n",
        "          action = np.random.choice(4)\n",
        "        # 숫자가 epsilon보다 크거나 같을 경우 q-value을 최대화하는 action을 선택\n",
        "        else:\n",
        "          action = np.argmax(self.q_values[state])\n",
        "        return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75i5h8kmr9Mp"
      },
      "source": [
        "OpenAI Gym에서의 Cliff Walking 환경을 로드하고 해당하는 환경을 살펴보기 위해 `render()` 메쏘드를 사용해봅니다.\n",
        "그리고 `env.nS` 및 `env.nA` 변수를 통해 해당 환경의 state 및 action 개수를 확인합니다.\n",
        "\n",
        "Cliff Walking 환경에서 각 state는 grid에서의 위치, action은 'up', 'right', 'down', 'left' 방향을 의미합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orqk6rcLg-uZ",
        "outputId": "bfcc9403-6721-4488-c2a4-b5fbd56024d7"
      },
      "source": [
        "env = CliffWalkingEnv() # Cliff Walking 환경 load\n",
        "env.render() # render() : Graphic User Interface (GUI)로 현재 진행상황을 출력하는 함수\n",
        "print ('Number of states: ', env.nS) # 해당 환경의 state\n",
        "print ('Number of actions :', env.nA) # action 개수 - '↑','↓','→','←'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Number of states:  48\n",
            "Number of actions : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og4Di2fGll2B"
      },
      "source": [
        "주어진 Q-value에서 greedy policy를 출력하는 QtoPolicy Class를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3H1tlkwnSVu"
      },
      "source": [
        "policy = QtoPolicy() # Q-value에서의 greedy 정책 출력을 위해 class 정의"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unvFLwuGsaEQ"
      },
      "source": [
        "Q-learning Class를 정의하고 5000 episode 동안 학습을 수행합니다.\n",
        "\n",
        "Gym 라이브러리의 환경에서는 `step(action)` 메쏘드를 통해 해당하는 time-step에서 action을 수행한 효과를 얻을 수 있습니다. 해당 메쏘드에서는 action을 수행하여 얻어지는 보상 (reward), 다음 상태 (next_state), done (episode 종료여부) 등이 출력으로 주어집니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "patW0RK5olea"
      },
      "source": [
        "agent_QL = QLearning() # Q-learning class 정의\n",
        "for ep in range(5000):\n",
        "    done = False\n",
        "    # reset() : 상태 초기화 - Step을 실행하다가 epsiode가 끝나서 이를 초기화해서 재시작해야할 때, 초기 State를 반환\n",
        "    state = env.reset() \n",
        "    action = agent_QL.act(state) # 앱실론 greedy 정책으로 Choose action\n",
        "    \n",
        "    ep_rewards = 0 # 총 누적 reward 변수 초기화\n",
        "    while not done:\n",
        "        # 알고리즘을 통해 얻어낸 Action을 1 step 수행한 후, 현재 state와 reward 등의 정보를 반환\n",
        "        # Step()함수의 return : tuple 형태의 (next_state, reward, done, info)를 반환\n",
        "        next_state, reward, done, info = env.step(action) \n",
        "\n",
        "        # 앱실론 greedy 정책으로 다음 action choose\n",
        "        next_action = agent_QL.act(next_state) \n",
        "\n",
        "        # 가치함수 update\n",
        "        agent_QL.update(state, action, reward, next_state, next_action) \n",
        "        \n",
        "        ep_rewards += reward # 얻은 reward 더해줌\n",
        "        state = next_state # 상태 update\n",
        "        action = next_action # action update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEtS85bXLxLW",
        "outputId": "fe9224dd-8e8b-4f7f-8b32-c50d4c974ec1"
      },
      "source": [
        "print('Q-learning reward :')\n",
        "print(ep_rewards)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-learning reward :\n",
            "-122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGnPP-qAsoAY"
      },
      "source": [
        "Sarsa에 대해서도 같은 방식으로 학습을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv2LB-xEqYL4"
      },
      "source": [
        "agent_Sa = Sarsa()\n",
        "for ep in range(5000):\n",
        "    done = False\n",
        "    # reset() : 상태 초기화 - Step을 실행하다가 epsiode가 끝나서 이를 초기화해서 재시작해야할 때, 초기 State를 반환\n",
        "    state = env.reset()\n",
        "    action = agent_Sa.act(state) # 앱실론 greedy 정책으로 Choose action\n",
        "    \n",
        "    ep_rewards = 0 # 총 누적 reward 변수 초기화\n",
        "    while not done:\n",
        "        # 알고리즘을 통해 얻어낸 Action을 1 step 수행한 후, 현재 state와 reward 등의 정보를 반환\n",
        "        # Step()함수의 return : tuple 형태의 (next_state, reward, done, info)를 반환\n",
        "        next_state, reward, done, info = env.step(action) # action 수행\n",
        "\n",
        "        # 앱실론 greedy 정책으로 다음 action choose\n",
        "        next_action = agent_Sa.act(next_state) \n",
        "\n",
        "        # 가치함수 update\n",
        "        agent_Sa.update(state, action, reward, next_state, next_action) \n",
        "        \n",
        "        ep_rewards += reward # 얻은 reward 더해줌\n",
        "        state = next_state # 상태 update\n",
        "        action = next_action # action update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys3w20F7LmNo",
        "outputId": "a8e03af6-07d0-471e-fe8d-b4a3b1ddcddf"
      },
      "source": [
        "print('Sarsa reward :')\n",
        "print(ep_rewards)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sarsa reward :\n",
            "-19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox-__wN3s3LR"
      },
      "source": [
        "학습된 Q-value를 이용하여 학습된 정책을 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqHh3TRcs3dh",
        "outputId": "7ac0e7a0-8bfa-4cdc-9872-def273005bd2"
      },
      "source": [
        "print('Learned policy by Q-learning')\n",
        "policy.printPolicy(agent_QL.q_values)\n",
        "print('Learned policy by Sarsa')\n",
        "policy.printPolicy(agent_Sa.q_values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned policy by Q-learning\n",
            "[['→' '↑' '→' '↓' '→' '↓' '→' '↓' '→' '→' '↓' '↓']\n",
            " ['←' '→' '→' '↓' '→' '→' '→' '→' '→' '→' '↓' '↓']\n",
            " ['→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '↓']\n",
            " ['↑' '←' '←' '←' '←' '←' '←' '←' '←' '←' '←' '↑']]\n",
            "\n",
            "Learned policy by Sarsa\n",
            "[['→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '→' '↓']\n",
            " ['→' '↑' '→' '→' '→' '→' '→' '→' '→' '→' '→' '↓']\n",
            " ['↑' '←' '↑' '↑' '→' '↑' '↑' '↑' '↑' '→' '→' '↓']\n",
            " ['↑' '←' '←' '←' '←' '←' '←' '←' '←' '←' '←' '↑']]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NFlcKhvoq4u"
      },
      "source": [
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O7_-casNlbr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}